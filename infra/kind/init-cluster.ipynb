{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kubernetes in /home/guillaume/.local/lib/python3.10/site-packages (23.3.0)\n",
      "Requirement already satisfied: tqdm in /home/guillaume/.local/lib/python3.10/site-packages (4.66.1)\n",
      "Requirement already satisfied: pandas in /home/guillaume/.local/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: nbformat in /home/guillaume/.local/lib/python3.10/site-packages (5.7.1)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/lib/python3/dist-packages (from kubernetes) (2020.6.20)\n",
      "Requirement already satisfied: requests-oauthlib in /home/guillaume/.local/lib/python3.10/site-packages (from kubernetes) (1.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/lib/python3/dist-packages (from kubernetes) (1.26.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/guillaume/.local/lib/python3.10/site-packages (from kubernetes) (2.8.2)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /usr/lib/python3/dist-packages (from kubernetes) (5.4.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/guillaume/.local/lib/python3.10/site-packages (from kubernetes) (1.4.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/lib/python3/dist-packages (from kubernetes) (59.6.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/guillaume/.local/lib/python3.10/site-packages (from kubernetes) (2.22.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from kubernetes) (1.16.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from kubernetes) (2.25.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/lib/python3/dist-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/guillaume/.local/lib/python3.10/site-packages (from nbformat) (3.0.2)\n",
      "Requirement already satisfied: fastjsonschema in /home/guillaume/.local/lib/python3.10/site-packages (from nbformat) (2.16.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /home/guillaume/.local/lib/python3.10/site-packages (from nbformat) (5.8.0)\n",
      "Requirement already satisfied: jupyter-core in /home/guillaume/.local/lib/python3.10/site-packages (from nbformat) (5.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/guillaume/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/guillaume/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/guillaume/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes) (5.3.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/guillaume/.local/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (0.19.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/lib/python3/dist-packages (from jupyter-core->nbformat) (2.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/guillaume/.local/lib/python3.10/site-packages (from requests-oauthlib->kubernetes) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/guillaume/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes) (0.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%run ../common/common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution: Nodes should have been already in `cluster.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind delete cluster\n",
      "kind delete cluster\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1465080/3770676444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kind delete cluster\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kind create cluster --config=cluster.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sleep 30\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1465080/2823847504.py\u001b[0m in \u001b[0;36mrun_command\u001b[0;34m(command, shell, log)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprocesses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    969\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    972\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1861\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kind'"
     ]
    }
   ],
   "source": [
    "run_command(\"kind delete cluster\", shell=False)\n",
    "run_command(\"kind create cluster --config=cluster.yaml\", shell=False)\n",
    "run_command(\"sleep 30\", shell=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../common/common_modules.sh\n",
      "../common/common_modules.sh\n",
      "Log 0 - 2023-08-19 17:58:13 : Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding\n",
      "Log 0 - 2023-08-19 17:58:13 : clusterrolebinding.rbac.authorization.k8s.io/manager-full created\n",
      "Log 0 - 2023-08-19 17:58:15 : customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created\n",
      "Log 0 - 2023-08-19 17:58:15 : customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created\n",
      "Log 0 - 2023-08-19 17:58:15 : customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created\n",
      "Log 0 - 2023-08-19 17:58:15 : customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created\n",
      "Log 0 - 2023-08-19 17:58:16 : customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created\n",
      "Log 0 - 2023-08-19 17:58:16 : customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created\n",
      "Log 0 - 2023-08-19 17:58:16 : namespace/cert-manager created\n",
      "Log 0 - 2023-08-19 17:58:16 : serviceaccount/cert-manager-cainjector created\n",
      "Log 0 - 2023-08-19 17:58:16 : serviceaccount/cert-manager created\n",
      "Log 0 - 2023-08-19 17:58:16 : serviceaccount/cert-manager-webhook created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrole.rbac.authorization.k8s.io/cert-manager-view created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrole.rbac.authorization.k8s.io/cert-manager-edit created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\n",
      "Log 0 - 2023-08-19 17:58:16 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\n",
      "Log 0 - 2023-08-19 17:58:16 : role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\n",
      "Log 0 - 2023-08-19 17:58:16 : role.rbac.authorization.k8s.io/cert-manager:leaderelection created\n",
      "Log 0 - 2023-08-19 17:58:16 : role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\n",
      "Log 0 - 2023-08-19 17:58:16 : rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\n",
      "Log 0 - 2023-08-19 17:58:16 : rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created\n",
      "Log 0 - 2023-08-19 17:58:16 : rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\n",
      "Log 0 - 2023-08-19 17:58:16 : service/cert-manager created\n",
      "Log 0 - 2023-08-19 17:58:16 : service/cert-manager-webhook created\n",
      "Log 0 - 2023-08-19 17:58:16 : deployment.apps/cert-manager-cainjector created\n",
      "Log 0 - 2023-08-19 17:58:16 : deployment.apps/cert-manager created\n",
      "Log 0 - 2023-08-19 17:58:16 : deployment.apps/cert-manager-webhook created\n",
      "Log 0 - 2023-08-19 17:58:16 : mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\n",
      "Log 0 - 2023-08-19 17:58:16 : validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\n",
      "Log 0 - 2023-08-19 17:58:16 : Waiting for deployment \"cert-manager-webhook\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "Log 0 - 2023-08-19 17:58:26 : deployment \"cert-manager-webhook\" successfully rolled out\n",
      "Log 0 - 2023-08-19 17:58:28 : namespace/flink-operator-system created\n",
      "Log 0 - 2023-08-19 17:58:28 : customresourcedefinition.apiextensions.k8s.io/flinkclusters.flinkoperator.k8s.io created\n",
      "Log 0 - 2023-08-19 17:58:28 : serviceaccount/flink-operator-controller-manager created\n",
      "Log 0 - 2023-08-19 17:58:28 : role.rbac.authorization.k8s.io/flink-operator-leader-election-role created\n",
      "Log 0 - 2023-08-19 17:58:28 : clusterrole.rbac.authorization.k8s.io/flink-operator-manager-role created\n",
      "Log 0 - 2023-08-19 17:58:28 : clusterrole.rbac.authorization.k8s.io/flink-operator-metrics-reader created\n",
      "Log 0 - 2023-08-19 17:58:28 : clusterrole.rbac.authorization.k8s.io/flink-operator-proxy-role created\n",
      "Log 0 - 2023-08-19 17:58:28 : rolebinding.rbac.authorization.k8s.io/flink-operator-leader-election-rolebinding created\n",
      "Log 0 - 2023-08-19 17:58:28 : clusterrolebinding.rbac.authorization.k8s.io/flink-operator-manager-rolebinding created\n",
      "Log 0 - 2023-08-19 17:58:28 : clusterrolebinding.rbac.authorization.k8s.io/flink-operator-proxy-rolebinding created\n",
      "Log 0 - 2023-08-19 17:58:28 : service/flink-operator-controller-manager-metrics-service created\n",
      "Log 0 - 2023-08-19 17:58:28 : service/flink-operator-webhook-service created\n",
      "Log 0 - 2023-08-19 17:58:28 : deployment.apps/flink-operator-controller-manager created\n",
      "Log 0 - 2023-08-19 17:58:28 : certificate.cert-manager.io/flink-operator-serving-cert created\n",
      "Log 0 - 2023-08-19 17:58:28 : issuer.cert-manager.io/flink-operator-selfsigned-issuer created\n",
      "Log 0 - 2023-08-19 17:58:28 : mutatingwebhookconfiguration.admissionregistration.k8s.io/flink-operator-mutating-webhook-configuration created\n",
      "Log 0 - 2023-08-19 17:58:29 : validatingwebhookconfiguration.admissionregistration.k8s.io/flink-operator-validating-webhook-configuration created\n",
      "Log 0 - 2023-08-19 17:58:29 : namespace/manager created\n",
      "Log 0 - 2023-08-19 17:58:29 : namespace/local-path-storage unchanged\n",
      "Log 0 - 2023-08-19 17:58:29 : serviceaccount/local-path-provisioner-service-account unchanged\n",
      "Log 0 - 2023-08-19 17:58:29 : clusterrole.rbac.authorization.k8s.io/local-path-provisioner-role unchanged\n",
      "Log 0 - 2023-08-19 17:58:29 : clusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind unchanged\n",
      "Log 0 - 2023-08-19 17:58:29 : deployment.apps/local-path-provisioner configured\n",
      "Log 0 - 2023-08-19 17:58:29 : storageclass.storage.k8s.io/local-path created\n",
      "Log 0 - 2023-08-19 17:58:30 : configmap/local-path-config configured\n",
      "Log 0 - 2023-08-19 17:58:31 : error: the path \"./cm-local-path.yaml\" does not exist\n",
      "Log 0 - 2023-08-19 17:58:31 : persistentvolumeclaim/pvc-manager-minio created\n",
      "Log 0 - 2023-08-19 17:59:01 : \"minio\" already exists with the same configuration, skipping\n",
      "Log 0 - 2023-08-19 17:59:01 : Hang tight while we grab the latest from your chart repositories...\n",
      "Log 0 - 2023-08-19 17:59:01 : index.go:346: skipping loading invalid entry for chart \"flink-operator\" \"v1beta1\" from https://googlecloudplatform.github.io/flink-on-k8s-operator/: validation: chart.metadata.version \"v1beta1\" is invalid\n",
      "Log 0 - 2023-08-19 17:59:01 : index.go:346: skipping loading invalid entry for chart \"flink-operator\" \"v1alpha1\" from https://googlecloudplatform.github.io/flink-on-k8s-operator/: validation: chart.metadata.version \"v1alpha1\" is invalid\n",
      "Log 0 - 2023-08-19 17:59:01 : ...Successfully got an update from the \"cloudhut\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:01 : ...Successfully got an update from the \"flink-operator-repo\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:01 : ...Successfully got an update from the \"confluentinc\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:01 : ...Successfully got an update from the \"kube-state-metrics\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:01 : ...Unable to get an update from the \"incubator\" chart repository (http://storage.googleapis.com/kubernetes-charts-incubator):\n",
      "Log 0 - 2023-08-19 17:59:01 : failed to fetch http://storage.googleapis.com/kubernetes-charts-incubator/index.yaml : 403 Forbidden\n",
      "Log 0 - 2023-08-19 17:59:01 : ...Successfully got an update from the \"minio\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:01 : ...Successfully got an update from the \"openkruise\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:03 : ...Successfully got an update from the \"argo\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:03 : ...Successfully got an update from the \"stable\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:03 : ...Successfully got an update from the \"bitnami\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:03 : ...Successfully got an update from the \"grafana\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:05 : ...Successfully got an update from the \"prometheus-community\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:07 : ...Successfully got an update from the \"my-repo\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:07 : Update Complete. ⎈Happy Helming!⎈\n",
      "Log 0 - 2023-08-19 17:59:08 : NAME: minio\n",
      "Log 0 - 2023-08-19 17:59:08 : LAST DEPLOYED: Sat Aug 19 17:59:08 2023\n",
      "Log 0 - 2023-08-19 17:59:08 : NAMESPACE: manager\n",
      "Log 0 - 2023-08-19 17:59:08 : STATUS: deployed\n",
      "Log 0 - 2023-08-19 17:59:08 : REVISION: 1\n",
      "Log 0 - 2023-08-19 17:59:08 : TEST SUITE: None\n",
      "Log 0 - 2023-08-19 17:59:08 : NOTES:\n",
      "Log 0 - 2023-08-19 17:59:08 : Minio can be accessed via port 9000 on the following DNS name from within your cluster:\n",
      "Log 0 - 2023-08-19 17:59:08 : minio.manager.svc.cluster.local\n",
      "Log 0 - 2023-08-19 17:59:08 : \n",
      "Log 0 - 2023-08-19 17:59:08 : To access Minio from localhost, run the below commands:\n",
      "Log 0 - 2023-08-19 17:59:08 : \n",
      "Log 0 - 2023-08-19 17:59:08 : 1. export POD_NAME=$(kubectl get pods --namespace manager -l \"release=minio\" -o jsonpath=\"{.items[0].metadata.name}\")\n",
      "Log 0 - 2023-08-19 17:59:08 : \n",
      "Log 0 - 2023-08-19 17:59:08 : 2. kubectl port-forward $POD_NAME 9000 --namespace manager\n",
      "Log 0 - 2023-08-19 17:59:08 : \n",
      "Log 0 - 2023-08-19 17:59:08 : Read more about port forwarding here: http://kubernetes.io/docs/user-guide/kubectl/kubectl_port-forward/\n",
      "Log 0 - 2023-08-19 17:59:08 : \n",
      "Log 0 - 2023-08-19 17:59:08 : You can now access Minio server on http://localhost:9000. Follow the below steps to connect to Minio server with mc client:\n",
      "Log 0 - 2023-08-19 17:59:08 : \n",
      "Log 0 - 2023-08-19 17:59:08 : 1. Download the Minio mc client - https://docs.minio.io/docs/minio-client-quickstart-guide\n",
      "Log 0 - 2023-08-19 17:59:08 : \n",
      "Log 0 - 2023-08-19 17:59:08 : 2. Get the ACCESS_KEY=$(kubectl get secret minio -o jsonpath=\"{.data.accesskey}\" | base64 --decode) and the SECRET_KEY=$(kubectl get secret minio -o jsonpath=\"{.data.secretkey}\" | base64 --decode)\n",
      "Log 0 - 2023-08-19 17:59:08 : \n",
      "Log 0 - 2023-08-19 17:59:08 : 3. mc alias set minio-local http://localhost:9000 \"$ACCESS_KEY\" \"$SECRET_KEY\" --api s3v4\n",
      "Log 0 - 2023-08-19 17:59:08 : \n",
      "Log 0 - 2023-08-19 17:59:08 : 4. mc ls minio-local\n",
      "Log 0 - 2023-08-19 17:59:08 : \n",
      "Log 0 - 2023-08-19 17:59:08 : Alternately, you can use your browser or the Minio SDK to access the server - https://docs.minio.io/categories/17\n",
      "Log 0 - 2023-08-19 17:59:08 : \"prometheus-community\" already exists with the same configuration, skipping\n",
      "Log 0 - 2023-08-19 17:59:08 : Hang tight while we grab the latest from your chart repositories...\n",
      "Log 0 - 2023-08-19 17:59:08 : ...Successfully got an update from the \"cloudhut\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:08 : ...Successfully got an update from the \"openkruise\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:08 : ...Successfully got an update from the \"confluentinc\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:08 : ...Successfully got an update from the \"kube-state-metrics\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:08 : ...Successfully got an update from the \"minio\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:09 : index.go:346: skipping loading invalid entry for chart \"flink-operator\" \"v1beta1\" from https://googlecloudplatform.github.io/flink-on-k8s-operator/: validation: chart.metadata.version \"v1beta1\" is invalid\n",
      "Log 0 - 2023-08-19 17:59:09 : index.go:346: skipping loading invalid entry for chart \"flink-operator\" \"v1alpha1\" from https://googlecloudplatform.github.io/flink-on-k8s-operator/: validation: chart.metadata.version \"v1alpha1\" is invalid\n",
      "Log 0 - 2023-08-19 17:59:09 : ...Successfully got an update from the \"flink-operator-repo\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:09 : ...Unable to get an update from the \"incubator\" chart repository (http://storage.googleapis.com/kubernetes-charts-incubator):\n",
      "Log 0 - 2023-08-19 17:59:09 : failed to fetch http://storage.googleapis.com/kubernetes-charts-incubator/index.yaml : 403 Forbidden\n",
      "Log 0 - 2023-08-19 17:59:09 : ...Successfully got an update from the \"argo\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:09 : ...Successfully got an update from the \"grafana\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:10 : ...Successfully got an update from the \"prometheus-community\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:15 : ...Successfully got an update from the \"stable\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:18 : ...Unable to get an update from the \"bitnami\" chart repository (https://charts.bitnami.com/bitnami):\n",
      "Log 0 - 2023-08-19 17:59:18 : Get \"https://charts.bitnami.com/bitnami/index.yaml\": dial tcp: lookup charts.bitnami.com on 127.0.0.53:53: read udp 127.0.0.1:36491->127.0.0.53:53: i/o timeout\n",
      "Log 0 - 2023-08-19 17:59:18 : ...Unable to get an update from the \"my-repo\" chart repository (https://charts.bitnami.com/bitnami):\n",
      "Log 0 - 2023-08-19 17:59:18 : Get \"https://charts.bitnami.com/bitnami/index.yaml\": dial tcp: lookup charts.bitnami.com on 127.0.0.53:53: read udp 127.0.0.1:36491->127.0.0.53:53: i/o timeout\n",
      "Log 0 - 2023-08-19 17:59:18 : Update Complete. ⎈Happy Helming!⎈\n",
      "Log 0 - 2023-08-19 17:59:39 : NAME: prom\n",
      "Log 0 - 2023-08-19 17:59:39 : LAST DEPLOYED: Sat Aug 19 17:59:22 2023\n",
      "Log 0 - 2023-08-19 17:59:39 : NAMESPACE: manager\n",
      "Log 0 - 2023-08-19 17:59:39 : STATUS: deployed\n",
      "Log 0 - 2023-08-19 17:59:39 : REVISION: 1\n",
      "Log 0 - 2023-08-19 17:59:39 : NOTES:\n",
      "Log 0 - 2023-08-19 17:59:39 : kube-prometheus-stack has been installed. Check its status by running:\n",
      "Log 0 - 2023-08-19 17:59:39 : kubectl --namespace manager get pods -l \"release=prom\"\n",
      "Log 0 - 2023-08-19 17:59:39 : \n",
      "Log 0 - 2023-08-19 17:59:39 : Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.\n",
      "Log 0 - 2023-08-19 17:59:39 : podmonitor.monitoring.coreos.com/flink-pod-monitor created\n",
      "Log 0 - 2023-08-19 17:59:39 : \"grafana\" already exists with the same configuration, skipping\n",
      "Log 0 - 2023-08-19 17:59:39 : Hang tight while we grab the latest from your chart repositories...\n",
      "Log 0 - 2023-08-19 17:59:42 : ...Successfully got an update from the \"argo\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:44 : ...Successfully got an update from the \"cloudhut\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:44 : ...Successfully got an update from the \"confluentinc\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:44 : ...Successfully got an update from the \"openkruise\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:44 : index.go:346: skipping loading invalid entry for chart \"flink-operator\" \"v1beta1\" from https://googlecloudplatform.github.io/flink-on-k8s-operator/: validation: chart.metadata.version \"v1beta1\" is invalid\n",
      "Log 0 - 2023-08-19 17:59:44 : index.go:346: skipping loading invalid entry for chart \"flink-operator\" \"v1alpha1\" from https://googlecloudplatform.github.io/flink-on-k8s-operator/: validation: chart.metadata.version \"v1alpha1\" is invalid\n",
      "Log 0 - 2023-08-19 17:59:44 : ...Successfully got an update from the \"flink-operator-repo\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:44 : ...Successfully got an update from the \"kube-state-metrics\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:44 : ...Successfully got an update from the \"minio\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:44 : ...Unable to get an update from the \"incubator\" chart repository (http://storage.googleapis.com/kubernetes-charts-incubator):\n",
      "Log 0 - 2023-08-19 17:59:44 : failed to fetch http://storage.googleapis.com/kubernetes-charts-incubator/index.yaml : 403 Forbidden\n",
      "Log 0 - 2023-08-19 17:59:47 : ...Successfully got an update from the \"prometheus-community\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:47 : ...Successfully got an update from the \"grafana\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:51 : ...Successfully got an update from the \"bitnami\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:51 : ...Successfully got an update from the \"my-repo\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:55 : ...Successfully got an update from the \"stable\" chart repository\n",
      "Log 0 - 2023-08-19 17:59:55 : Update Complete. ⎈Happy Helming!⎈\n",
      "Log 0 - 2023-08-19 17:59:55 : Release \"loki\" does not exist. Installing it now.\n",
      "Log 0 - 2023-08-19 17:59:57 : W0819 17:59:57.597022   27763 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n",
      "Log 0 - 2023-08-19 17:59:57 : W0819 17:59:57.597929   27763 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n",
      "Log 0 - 2023-08-19 17:59:57 : W0819 17:59:57.620628   27763 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n",
      "Log 0 - 2023-08-19 17:59:57 : W0819 17:59:57.620657   27763 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n",
      "Log 0 - 2023-08-19 17:59:57 : NAME: loki\n",
      "Log 0 - 2023-08-19 17:59:57 : LAST DEPLOYED: Sat Aug 19 17:59:57 2023\n",
      "Log 0 - 2023-08-19 17:59:57 : NAMESPACE: manager\n",
      "Log 0 - 2023-08-19 17:59:57 : STATUS: deployed\n",
      "Log 0 - 2023-08-19 17:59:57 : REVISION: 1\n",
      "Log 0 - 2023-08-19 17:59:57 : NOTES:\n",
      "Log 0 - 2023-08-19 17:59:57 : The Loki stack has been deployed to your cluster. Loki can now be added as a datasource in Grafana.\n",
      "Log 0 - 2023-08-19 17:59:57 : \n",
      "Log 0 - 2023-08-19 17:59:57 : See http://docs.grafana.org/features/datasources/loki/ for more detail.\n",
      "Log 0 - 2023-08-19 18:00:08 : namespace/kafka created\n",
      "Log 0 - 2023-08-19 18:00:10 : rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-entity-operator-delegation created\n",
      "Log 0 - 2023-08-19 18:00:10 : customresourcedefinition.apiextensions.k8s.io/strimzipodsets.core.strimzi.io created\n",
      "Log 0 - 2023-08-19 18:00:10 : clusterrole.rbac.authorization.k8s.io/strimzi-kafka-client created\n",
      "Log 0 - 2023-08-19 18:00:11 : customresourcedefinition.apiextensions.k8s.io/kafkausers.kafka.strimzi.io created\n",
      "Log 0 - 2023-08-19 18:00:11 : clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-broker-delegation created\n",
      "Log 0 - 2023-08-19 18:00:11 : configmap/strimzi-cluster-operator created\n",
      "Log 0 - 2023-08-19 18:00:11 : customresourcedefinition.apiextensions.k8s.io/kafkas.kafka.strimzi.io created\n",
      "Log 0 - 2023-08-19 18:00:11 : clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-namespaced created\n",
      "Log 0 - 2023-08-19 18:00:11 : customresourcedefinition.apiextensions.k8s.io/kafkatopics.kafka.strimzi.io created\n",
      "Log 0 - 2023-08-19 18:00:11 : customresourcedefinition.apiextensions.k8s.io/kafkaconnects.kafka.strimzi.io created\n",
      "Log 0 - 2023-08-19 18:00:11 : customresourcedefinition.apiextensions.k8s.io/kafkabridges.kafka.strimzi.io created\n",
      "Log 0 - 2023-08-19 18:00:11 : customresourcedefinition.apiextensions.k8s.io/kafkaconnectors.kafka.strimzi.io created\n",
      "Log 0 - 2023-08-19 18:00:11 : clusterrole.rbac.authorization.k8s.io/strimzi-entity-operator created\n",
      "Log 0 - 2023-08-19 18:00:11 : clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-global created\n",
      "Log 0 - 2023-08-19 18:00:11 : clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-client-delegation created\n",
      "Log 0 - 2023-08-19 18:00:11 : customresourcedefinition.apiextensions.k8s.io/kafkamirrormakers.kafka.strimzi.io created\n",
      "Log 0 - 2023-08-19 18:00:11 : clusterrole.rbac.authorization.k8s.io/strimzi-kafka-broker created\n",
      "Log 0 - 2023-08-19 18:00:11 : customresourcedefinition.apiextensions.k8s.io/kafkamirrormaker2s.kafka.strimzi.io created\n",
      "Log 0 - 2023-08-19 18:00:11 : customresourcedefinition.apiextensions.k8s.io/kafkarebalances.kafka.strimzi.io created\n",
      "Log 0 - 2023-08-19 18:00:11 : serviceaccount/strimzi-cluster-operator created\n",
      "Log 0 - 2023-08-19 18:00:11 : deployment.apps/strimzi-cluster-operator created\n",
      "Log 0 - 2023-08-19 18:00:12 : clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created\n",
      "Log 0 - 2023-08-19 18:00:12 : rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created\n",
      "Log 0 - 2023-08-19 18:00:12 : \"cloudhut\" already exists with the same configuration, skipping\n",
      "Log 0 - 2023-08-19 18:00:12 : Hang tight while we grab the latest from your chart repositories...\n",
      "Log 0 - 2023-08-19 18:00:12 : index.go:346: skipping loading invalid entry for chart \"flink-operator\" \"v1beta1\" from https://googlecloudplatform.github.io/flink-on-k8s-operator/: validation: chart.metadata.version \"v1beta1\" is invalid\n",
      "Log 0 - 2023-08-19 18:00:12 : index.go:346: skipping loading invalid entry for chart \"flink-operator\" \"v1alpha1\" from https://googlecloudplatform.github.io/flink-on-k8s-operator/: validation: chart.metadata.version \"v1alpha1\" is invalid\n",
      "Log 0 - 2023-08-19 18:00:12 : ...Successfully got an update from the \"flink-operator-repo\" chart repository\n",
      "Log 0 - 2023-08-19 18:00:12 : ...Successfully got an update from the \"cloudhut\" chart repository\n",
      "Log 0 - 2023-08-19 18:00:12 : ...Successfully got an update from the \"minio\" chart repository\n",
      "Log 0 - 2023-08-19 18:00:12 : ...Unable to get an update from the \"incubator\" chart repository (http://storage.googleapis.com/kubernetes-charts-incubator):\n",
      "Log 0 - 2023-08-19 18:00:12 : failed to fetch http://storage.googleapis.com/kubernetes-charts-incubator/index.yaml : 403 Forbidden\n",
      "Log 0 - 2023-08-19 18:00:14 : ...Successfully got an update from the \"kube-state-metrics\" chart repository\n",
      "Log 0 - 2023-08-19 18:00:16 : ...Successfully got an update from the \"confluentinc\" chart repository\n",
      "Log 0 - 2023-08-19 18:00:19 : ...Successfully got an update from the \"openkruise\" chart repository\n",
      "Log 0 - 2023-08-19 18:00:29 : ...Successfully got an update from the \"argo\" chart repository\n",
      "Log 0 - 2023-08-19 18:00:31 : ...Successfully got an update from the \"grafana\" chart repository\n",
      "Log 0 - 2023-08-19 18:00:34 : ...Successfully got an update from the \"prometheus-community\" chart repository\n",
      "Log 0 - 2023-08-19 18:00:37 : ...Successfully got an update from the \"bitnami\" chart repository\n",
      "Log 0 - 2023-08-19 18:00:38 : ...Successfully got an update from the \"my-repo\" chart repository\n",
      "Log 0 - 2023-08-19 18:00:47 : ...Successfully got an update from the \"stable\" chart repository\n",
      "Log 0 - 2023-08-19 18:00:47 : Update Complete. ⎈Happy Helming!⎈\n",
      "Log 0 - 2023-08-19 18:00:49 : configmap/zeppelin-server-conf-map created\n",
      "Log 0 - 2023-08-19 18:00:49 : configmap/zeppelin-server-conf created\n",
      "Log 0 - 2023-08-19 18:00:49 : persistentvolumeclaim/zeppelin-volume created\n",
      "Log 0 - 2023-08-19 18:00:50 : deployment.apps/zeppelin-server created\n",
      "Log 0 - 2023-08-19 18:00:50 : service/zeppelin-server created\n",
      "Log 0 - 2023-08-19 18:00:50 : serviceaccount/zeppelin-server created\n",
      "Log 0 - 2023-08-19 18:00:50 : clusterrole.rbac.authorization.k8s.io/zeppelin-server-role created\n",
      "Log 0 - 2023-08-19 18:00:51 : rolebinding.rbac.authorization.k8s.io/zeppelin-server-role-binding created\n",
      "Log 0 - 2023-08-19 18:00:51 : ingress.networking.k8s.io/zeppelin created\n",
      "Log 0 - 2023-08-19 18:00:51 : ingress.networking.k8s.io/minio created\n",
      "Log 0 - 2023-08-19 18:00:51 : ingress.networking.k8s.io/grafana created\n",
      "Log 0 - 2023-08-19 18:00:51 : ingress.networking.k8s.io/prometheus created\n",
      "Log 0 - 2023-08-19 18:00:51 : ingress.networking.k8s.io/zeppelin unchanged\n",
      "namespace/ingress-nginx created\n",
      "serviceaccount/ingress-nginx created\n",
      "serviceaccount/ingress-nginx-admission created\n",
      "role.rbac.authorization.k8s.io/ingress-nginx created\n",
      "role.rbac.authorization.k8s.io/ingress-nginx-admission created\n",
      "clusterrole.rbac.authorization.k8s.io/ingress-nginx created\n",
      "clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created\n",
      "rolebinding.rbac.authorization.k8s.io/ingress-nginx created\n",
      "rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\n",
      "configmap/ingress-nginx-controller created\n",
      "service/ingress-nginx-controller created\n",
      "service/ingress-nginx-controller-admission created\n",
      "deployment.apps/ingress-nginx-controller created\n",
      "job.batch/ingress-nginx-admission-create created\n",
      "job.batch/ingress-nginx-admission-patch created\n",
      "ingressclass.networking.k8s.io/nginx created\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created\n"
     ]
    }
   ],
   "source": [
    "run_command('../common/common_modules.sh')\n",
    "!kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind-control-plane\t{'beta.kubernetes.io/arch': 'amd64', 'beta.kubernetes.io/os': 'linux', 'kubernetes.io/arch': 'amd64', 'kubernetes.io/hostname': 'kind-control-plane', 'kubernetes.io/os': 'linux', 'node-role.kubernetes.io/control-plane': '', 'node-role.kubernetes.io/master': '', 'node.kubernetes.io/exclude-from-external-load-balancers': ''}\n",
      "kind-worker\t{'beta.kubernetes.io/arch': 'amd64', 'beta.kubernetes.io/os': 'linux', 'ingress-ready': 'true', 'kubernetes.io/arch': 'amd64', 'kubernetes.io/hostname': 'kind-worker', 'kubernetes.io/os': 'linux', 'tier': 'manager'}\n",
      "kind-worker2\t{'beta.kubernetes.io/arch': 'amd64', 'beta.kubernetes.io/os': 'linux', 'kubernetes.io/arch': 'amd64', 'kubernetes.io/hostname': 'kind-worker2', 'kubernetes.io/os': 'linux', 'tier': 'jobmanager'}\n",
      "kind-worker3\t{'beta.kubernetes.io/arch': 'amd64', 'beta.kubernetes.io/os': 'linux', 'kubernetes.io/arch': 'amd64', 'kubernetes.io/hostname': 'kind-worker3', 'kubernetes.io/os': 'linux', 'tier': 'taskmanager'}\n"
     ]
    }
   ],
   "source": [
    "(manager_node, jobmanager_node, taskmanager_nodes) = get_label_nodes(ip_address=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             grafana\n",
      "Labels:           <none>\n",
      "Namespace:        manager\n",
      "Address:          \n",
      "Ingress Class:    <none>\n",
      "Default backend:  <default>\n",
      "Rules:\n",
      "  Host                        Path  Backends\n",
      "  ----                        ----  --------\n",
      "  grafana.127-0-0-1.sslip.io  \n",
      "                              /   prom-grafana:80 ()\n",
      "Annotations:                  kubernetes.io/ingress.class: nginx\n",
      "                              nginx.ingress.kubernetes.io/proxy-body-size: 0\n",
      "Events:                       <none>\n",
      "\n",
      "\n",
      "Name:             minio\n",
      "Labels:           <none>\n",
      "Namespace:        manager\n",
      "Address:          \n",
      "Ingress Class:    <none>\n",
      "Default backend:  <default>\n",
      "Rules:\n",
      "  Host                      Path  Backends\n",
      "  ----                      ----  --------\n",
      "  minio.127-0-0-1.sslip.io  \n",
      "                            /   minio:9000 (10.244.3.8:9000)\n",
      "Annotations:                kubernetes.io/ingress.class: nginx\n",
      "                            nginx.ingress.kubernetes.io/proxy-body-size: 0\n",
      "Events:                     <none>\n",
      "\n",
      "\n",
      "Name:             prometheus\n",
      "Labels:           <none>\n",
      "Namespace:        manager\n",
      "Address:          \n",
      "Ingress Class:    <none>\n",
      "Default backend:  <default>\n",
      "Rules:\n",
      "  Host                           Path  Backends\n",
      "  ----                           ----  --------\n",
      "  prometheus.127-0-0-1.sslip.io  \n",
      "                                 /   prom-kube-prometheus-stack-prometheus:9090 ()\n",
      "Annotations:                     kubernetes.io/ingress.class: nginx\n",
      "                                 nginx.ingress.kubernetes.io/proxy-body-size: 0\n",
      "Events:                          <none>\n",
      "\n",
      "\n",
      "Name:             zeppelin\n",
      "Labels:           <none>\n",
      "Namespace:        manager\n",
      "Address:          \n",
      "Ingress Class:    <none>\n",
      "Default backend:  <default>\n",
      "Rules:\n",
      "  Host                         Path  Backends\n",
      "  ----                         ----  --------\n",
      "  zeppelin.127-0-0-1.sslip.io  \n",
      "                               /   zeppelin-server:8080 (<none>)\n",
      "Annotations:                   kubernetes.io/ingress.class: nginx\n",
      "                               nginx.ingress.kubernetes.io/proxy-body-size: 0\n",
      "Events:                        <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe ing -n manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manager node: 172.18.0.2:30080\n",
      "Access to Minio: http://minio.127-0-0-1.nip.io:30080\n",
      "Access to Grafana: http://grafana.127-0-0-1.nip.io:30080\n",
      "Access to Prometheus: http://prometheus.127-0-0-1:30080\n",
      "Job manager address: 172.18.0.5\n",
      "Task manager addresses: 172.18.0.3\n"
     ]
    }
   ],
   "source": [
    "address = \"127-0-0-1\"\n",
    "print(\"Manager node: {}:30080\".format(manager_node))\n",
    "print(\"Access to Minio: http://minio.{}.nip.io:30080\".format(address))\n",
    "print(\"Access to Grafana: http://grafana.{}.nip.io:30080\".format(address))\n",
    "print(\"Access to Prometheus: http://prometheus.{}:30080\".format(address))\n",
    "print(\"Job manager address: {}\".format(jobmanager_node))\n",
    "print(\"Task manager addresses: {}\".format(taskmanager_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access to Minio: http://172.18.0.2:30900\n",
      "Access to Grafana: http://172.18.0.2:30300\n",
      "Access to Prometheus: http://172.18.0.2:30090\n",
      "Job manager address: 172.18.0.5\n",
      "Task manager addresses: 172.18.0.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Access to Minio: http://{}:30900\".format(manager_node))\n",
    "print(\"Access to Grafana: http://{}:30300\".format(manager_node))\n",
    "print(\"Access to Prometheus: http://{}:30090\".format(manager_node))\n",
    "print(\"Job manager address: {}\".format(jobmanager_node))\n",
    "print(\"Task manager addresses: {}\".format(taskmanager_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/guillaume/research/gepiciad/StreamBed/infra/kind\n",
      "pod/zeppelin-server-75767d75dd-pbz7w condition met\n",
      "{\"status\":\"OK\",\"message\":\"\",\"body\":{\"id\":\"flink\",\"name\":\"flink\",\"group\":\"flink\",\"properties\":{\"FLINK_HOME\":{\"name\":\"FLINK_HOME\",\"value\":\"\",\"type\":\"string\",\"description\":\"Location of flink distribution\"},\"HADOOP_CONF_DIR\":{\"name\":\"HADOOP_CONF_DIR\",\"value\":\"\",\"type\":\"string\",\"description\":\"Location of hadoop conf (core-site.xml, hdfs-site.xml and etc.)\"},\"HIVE_CONF_DIR\":{\"name\":\"HIVE_CONF_DIR\",\"value\":\"\",\"type\":\"string\",\"description\":\"Location of hive conf (hive-site.xml)\"},\"flink.execution.mode\":{\"name\":\"flink.execution.mode\",\"value\":\"local\",\"type\":\"string\",\"description\":\"Execution mode, it could be local|remote|yarn\"},\"flink.execution.remote.host\":{\"name\":\"flink.execution.remote.host\",\"value\":\"\",\"type\":\"string\",\"description\":\"Host name of running JobManager. Only used for remote mode\"},\"flink.execution.remote.port\":{\"name\":\"flink.execution.remote.port\",\"value\":\"\",\"type\":\"number\",\"description\":\"Port of running JobManager. Only used for remote mode\"},\"jobmanager.memory.process.size\":{\"name\":\"jobmanager.memory.process.size\",\"value\":\"1024m\",\"type\":\"text\",\"description\":\"Memory for JobManager, e.g. 1024m\"},\"taskmanager.memory.process.size\":{\"name\":\"taskmanager.memory.process.size\",\"value\":\"1024m\",\"type\":\"text\",\"description\":\"Memory for TaskManager, e.g. 1024m\"},\"taskmanager.numberOfTaskSlots\":{\"name\":\"taskmanager.numberOfTaskSlots\",\"value\":\"1\",\"type\":\"number\",\"description\":\"Number of slot per TaskManager\"},\"local.number-taskmanager\":{\"name\":\"local.number-taskmanager\",\"value\":\"4\",\"type\":\"number\",\"description\":\"Number of TaskManager in local mode\"},\"yarn.application.name\":{\"name\":\"yarn.application.name\",\"value\":\"Zeppelin Flink Session\",\"type\":\"string\",\"description\":\"Yarn app name\"},\"yarn.application.queue\":{\"name\":\"yarn.application.queue\",\"value\":\"default\",\"type\":\"string\",\"description\":\"Yarn queue name\"},\"zeppelin.flink.uiWebUrl\":{\"name\":\"zeppelin.flink.uiWebUrl\",\"value\":\"\",\"type\":\"string\",\"description\":\"User specified Flink JobManager url, it could be used in remote mode where Flink cluster is already started, or could be used as url template, e.g. https://knox-server:8443/gateway/cluster-topo/yarn/proxy/{{applicationId}}/ where {{applicationId}} would be replaced with yarn app id\"},\"zeppelin.flink.run.asLoginUser\":{\"name\":\"zeppelin.flink.run.asLoginUser\",\"value\":true,\"type\":\"checkbox\",\"description\":\"Whether run flink job as the zeppelin login user, it is only applied when running flink job in hadoop yarn cluster and shiro is enabled\"},\"flink.udf.jars\":{\"name\":\"flink.udf.jars\",\"value\":\"\",\"type\":\"string\",\"description\":\"Flink udf jars (comma separated), Zeppelin will register udfs in this jar for user automatically, these udf jars could be either local files or hdfs files if you have hadoop installed, the udf name is the class name\"},\"flink.udf.jars.packages\":{\"name\":\"flink.udf.jars.packages\",\"value\":\"\",\"type\":\"string\",\"description\":\"Packages (comma separated) that would be searched for the udf defined in `flink.udf.jars`\"},\"flink.execution.jars\":{\"name\":\"flink.execution.jars\",\"value\":\"\",\"type\":\"string\",\"description\":\"Additional user jars (comma separated), these jars could be either local files or hdfs files if you have hadoop installed\"},\"flink.execution.packages\":{\"name\":\"flink.execution.packages\",\"value\":\"\",\"type\":\"string\",\"description\":\"Additional user packages (comma separated), e.g. flink connector packages\"},\"zeppelin.flink.scala.color\":{\"name\":\"zeppelin.flink.scala.color\",\"value\":true,\"type\":\"checkbox\",\"description\":\"Whether display scala shell output in colorful format\"},\"zeppelin.flink.enableHive\":{\"name\":\"zeppelin.flink.enableHive\",\"value\":false,\"type\":\"checkbox\",\"description\":\"Whether enable hive\"},\"zeppelin.flink.hive.version\":{\"name\":\"zeppelin.flink.hive.version\",\"value\":\"2.3.4\",\"type\":\"string\",\"description\":\"Hive version that you would like to connect\"},\"zeppelin.flink.module.enableHive\":{\"name\":\"zeppelin.flink.module.enableHive\",\"value\":false,\"type\":\"checkbox\",\"description\":\"Whether enable hive module, hive udf take precedence over flink udf if hive module is enabled.\"},\"zeppelin.flink.printREPLOutput\":{\"name\":\"zeppelin.flink.printREPLOutput\",\"value\":true,\"type\":\"checkbox\",\"description\":\"Print REPL output\"},\"zeppelin.flink.maxResult\":{\"name\":\"zeppelin.flink.maxResult\",\"value\":\"1000\",\"type\":\"number\",\"description\":\"Max number of rows returned by sql interpreter.\"},\"zeppelin.pyflink.python\":{\"name\":\"zeppelin.pyflink.python\",\"value\":\"python\",\"type\":\"string\",\"description\":\"Python executable for pyflink\"},\"flink.interpreter.close.shutdown_cluster\":{\"name\":\"flink.interpreter.close.shutdown_cluster\",\"value\":true,\"type\":\"checkbox\",\"description\":\"Whether shutdown flink cluster when close interpreter\"},\"zeppelin.interpreter.close.cancel_job\":{\"name\":\"zeppelin.interpreter.close.cancel_job\",\"value\":true,\"type\":\"checkbox\",\"description\":\"Whether cancel flink job when closing interpreter\"},\"zeppelin.flink.job.check_interval\":{\"name\":\"zeppelin.flink.job.check_interval\",\"value\":\"1000\",\"type\":\"number\",\"description\":\"Check interval (in milliseconds) to check flink job progress\"},\"zeppelin.flink.concurrentBatchSql.max\":{\"name\":\"zeppelin.flink.concurrentBatchSql.max\",\"value\":\"10\",\"type\":\"number\",\"description\":\"Max concurrent sql of Batch Sql\"},\"zeppelin.flink.concurrentStreamSql.max\":{\"name\":\"zeppelin.flink.concurrentStreamSql.max\",\"value\":\"10\",\"type\":\"number\",\"description\":\"Max concurrent sql of Stream Sql\"}},\"status\":\"READY\",\"interpreterGroup\":[{\"name\":\"flink\",\"class\":\"org.apache.zeppelin.flink.FlinkInterpreter\",\"defaultInterpreter\":true,\"editor\":{\"language\":\"scala\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true}},{\"name\":\"bsql\",\"class\":\"org.apache.zeppelin.flink.FlinkBatchSqlInterpreter\",\"defaultInterpreter\":false,\"editor\":{\"language\":\"sql\",\"editOnDblClick\":false}},{\"name\":\"ssql\",\"class\":\"org.apache.zeppelin.flink.FlinkStreamSqlInterpreter\",\"defaultInterpreter\":false,\"editor\":{\"language\":\"sql\",\"editOnDblClick\":false}},{\"name\":\"pyflink\",\"class\":\"org.apache.zeppelin.flink.PyFlinkInterpreter\",\"defaultInterpreter\":false,\"editor\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true}},{\"name\":\"ipyflink\",\"class\":\"org.apache.zeppelin.flink.IPyFlinkInterpreter\",\"defaultInterpreter\":false,\"editor\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true}}],\"dependencies\":[],\"option\":{\"remote\":true,\"port\":-1,\"perNote\":\"isolated\",\"perUser\":\"\",\"isExistingProcess\":false,\"setPermission\":false,\"owners\":[],\"isUserImpersonate\":false}}}../../experiments/zeppelin/xp_datagen_2HY61EX49.zpln\n",
      "{\"status\":\"OK\"}../../experiments/zeppelin/xp_intro_init_kafka_2J1RECAWT.zpln\n",
      "{\"status\":\"OK\"}../../experiments/zeppelin/xp_intro_q11_kafka_2J19ME24J.zpln\n",
      "{\"status\":\"OK\"}../../experiments/zeppelin/xp_intro_q11_kafka_custom_ratelimit_2HX1W7GWY.zpln\n",
      "{\"status\":\"OK\"}../../experiments/zeppelin/xp_intro_q1_datagen_2HYTNPVD5.zpln\n",
      "{\"status\":\"OK\"}../../experiments/zeppelin/xp_intro_q1_kafka_custom_ratelimit_2HWFJB75E.zpln\n",
      "{\"status\":\"OK\"}../../experiments/zeppelin/xp_intro_q2_datagen_2J1SBKHM2.zpln\n",
      "{\"status\":\"OK\"}../../experiments/zeppelin/xp_intro_q2_kafka_custom_ratelimit_2HYRX196Q.zpln\n",
      "{\"status\":\"OK\"}../../experiments/zeppelin/xp_intro_q5_kafka_2HYPCFQQ5.zpln\n",
      "{\"status\":\"OK\"}../../experiments/zeppelin/xp_intro_q5_kafka_custom_ratelimit_2HXS4HNMD.zpln\n",
      "{\"status\":\"OK\"}../../experiments/zeppelin/xp_intro_q8_kafka_2HY93ZEVY.zpln\n",
      "{\"status\":\"OK\"}../../experiments/zeppelin/xp_intro_q8_kafka_custom_ratelimit_2HXJ6DCTN.zpln\n",
      "{\"status\":\"OK\"}{\"pluginId\":\"\",\"title\":\"Flink nexmark\",\"imported\":true,\"importedUri\":\"db/flink-nexmark\",\"importedUrl\":\"/d/quHGgdGVz/flink-nexmark\",\"slug\":\"flink-nexmark\",\"dashboardId\":26,\"folderId\":0,\"importedRevision\":1,\"revision\":1,\"description\":\"\",\"path\":\"\",\"removed\":false}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulted container \"zeppelin-server\" out of: zeppelin-server, zeppelin-server-gateway, dnsmasq, flink-downloader (init)\n",
      "--2023-08-19 16:06:58--  https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka_2.12/1.14.2/flink-sql-connector-kafka_2.12-1.14.2.jar\n",
      "Resolving repo.maven.apache.org (repo.maven.apache.org)... 151.101.8.215, 2a04:4e42:2::215\n",
      "Connecting to repo.maven.apache.org (repo.maven.apache.org)|151.101.8.215|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3703507 (3.5M) [application/java-archive]\n",
      "Saving to: ‘/tmp/flink-sql-connector-kafka_2.12-1.14.2.jar’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1% 2.74M 1s\n",
      "    50K .......... .......... .......... .......... ..........  2% 8.76M 1s\n",
      "   100K .......... .......... .......... .......... ..........  4% 4.06M 1s\n",
      "   150K .......... .......... .......... .......... ..........  5% 5.71M 1s\n",
      "   200K .......... .......... .......... .......... ..........  6% 16.7M 1s\n",
      "   250K .......... .......... .......... .......... ..........  8% 7.38M 1s\n",
      "   300K .......... .......... .......... .......... ..........  9% 20.5M 1s\n",
      "   350K .......... .......... .......... .......... .......... 11% 30.0M 0s\n",
      "   400K .......... .......... .......... .......... .......... 12% 13.4M 0s\n",
      "   450K .......... .......... .......... .......... .......... 13% 37.6M 0s\n",
      "   500K .......... .......... .......... .......... .......... 15% 11.8M 0s\n",
      "   550K .......... .......... .......... .......... .......... 16% 27.7M 0s\n",
      "   600K .......... .......... .......... .......... .......... 17% 18.9M 0s\n",
      "   650K .......... .......... .......... .......... .......... 19% 11.7M 0s\n",
      "   700K .......... .......... .......... .......... .......... 20% 87.9M 0s\n",
      "   750K .......... .......... .......... .......... .......... 22% 12.4M 0s\n",
      "   800K .......... .......... .......... .......... .......... 23%  100M 0s\n",
      "   850K .......... .......... .......... .......... .......... 24% 23.9M 0s\n",
      "   900K .......... .......... .......... .......... .......... 26% 23.9M 0s\n",
      "   950K .......... .......... .......... .......... .......... 27% 21.8M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 29% 69.5M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 30% 19.5M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 31% 19.4M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 33% 22.4M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 34% 68.7M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 35% 12.4M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 37% 78.8M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 38% 21.8M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 40% 11.8M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 41% 24.4M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 42% 87.0M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 44% 22.8M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 45% 12.8M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 47% 79.6M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 48% 18.5M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 49%  103M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 51% 20.9M 0s\n",
      "  1850K .......... .......... .......... .......... .......... 52% 23.1M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 53% 22.7M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 55% 71.5M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 56% 24.8M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 58% 7.50M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 59% 25.5M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 60% 20.7M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 62% 69.8M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 63% 23.1M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 64% 29.6M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 66% 13.3M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 67% 29.0M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 69% 22.3M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 70% 71.7M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 71% 26.3M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 73% 10.8M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 74% 86.6M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 76% 14.4M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 77% 44.2M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 78% 38.2M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 80% 16.0M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 81% 54.5M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 82% 11.1M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 84% 75.0M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 85% 28.4M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 87% 25.8M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 88% 95.3M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 89% 10.8M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 91% 81.6M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 92% 14.6M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 94% 86.9M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 95% 27.7M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 96% 27.1M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 98% 85.4M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 99% 15.4M 0s\n",
      "  3600K .......... ......                                     100% 41.4M=0.2s\n",
      "\n",
      "2023-08-19 16:06:58 (18.2 MB/s) - ‘/tmp/flink-sql-connector-kafka_2.12-1.14.2.jar’ saved [3703507/3703507]\n",
      "\n",
      "Defaulted container \"zeppelin-server\" out of: zeppelin-server, zeppelin-server-gateway, dnsmasq, flink-downloader (init)\n",
      "Defaulted container \"zeppelin-server\" out of: zeppelin-server, zeppelin-server-gateway, dnsmasq, flink-downloader (init)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 13767  100  6523  100  7244  41422  46001 --:--:-- --:--:-- --:--:-- 87132\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 14314  100    47  100 14267    933   276k --:--:-- --:--:-- --:--:--  279k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0   1423      0 --:--:-- --:--:-- --:--:--  1500\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 14109  100    47  100 14062   2087   609k --:--:-- --:--:-- --:--:--  626k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0   2938      0 --:--:-- --:--:-- --:--:--  3750\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 16701  100    47  100 16654   1985   687k --:--:-- --:--:-- --:--:--  709k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0   2521      0 --:--:-- --:--:-- --:--:--  3000\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 17626  100    47  100 17579   2060   752k --:--:-- --:--:-- --:--:--  782k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0   2718      0 --:--:-- --:--:-- --:--:--  3000\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 16815  100    47  100 16768   2043   711k --:--:-- --:--:-- --:--:--  746k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0   2295      0 --:--:-- --:--:-- --:--:--  2500\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 23637  100    47  100 23590   1970   965k --:--:-- --:--:-- --:--:-- 1003k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0   2727      0 --:--:-- --:--:-- --:--:--  3000\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 23012  100    47  100 22965   2601  1241k --:--:-- --:--:-- --:--:-- 1321k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0   1065      0 --:--:-- --:--:-- --:--:--  1153\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 17361  100    47  100 17314   1495   538k --:--:-- --:--:-- --:--:--  546k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0    471      0 --:--:-- --:--:-- --:--:--   483\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 23584  100    47  100 23537   1305   638k --:--:-- --:--:-- --:--:--  658k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0    597      0 --:--:-- --:--:-- --:--:--   625\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 36568  100    47  100 36521    395   300k --:--:-- --:--:-- --:--:--  302k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0    236      0 --:--:-- --:--:-- --:--:--   238\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 16094  100    47  100 16047   1042   347k --:--:-- --:--:-- --:--:--  357k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0    213      0 --:--:-- --:--:-- --:--:--   217\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 18941  100    47  100 18894    741   291k --:--:-- --:--:-- --:--:--  289k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0    409      0 --:--:-- --:--:-- --:--:--   416\n",
      "Defaulted container \"zeppelin-server\" out of: zeppelin-server, zeppelin-server-gateway, dnsmasq, flink-downloader (init)\n",
      "Defaulted container \"zeppelin-server\" out of: zeppelin-server, zeppelin-server-gateway, dnsmasq, flink-downloader (init)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 63518  100   261  100 63257   2975   704k --:--:-- --:--:-- --:--:--  712k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pwd\n",
    "export ZEPPELIN_POD=`kubectl get pods -n manager -o=name | grep zeppelin | sed \"s/^.\\{4\\}//\"` \n",
    "kubectl wait -n manager pod/$ZEPPELIN_POD --for condition=Ready --timeout=600s\n",
    "export MANAGER_NODE=`kubectl get nodes --selector=tier=manager -o jsonpath='{$.items[*].status.addresses[?(@.type==\"InternalIP\")].address}'`\n",
    "\n",
    "kubectl -n manager exec $ZEPPELIN_POD -- wget -P /tmp https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka_2.12/1.14.2/flink-sql-connector-kafka_2.12-1.14.2.jar\n",
    "# nexmark and the rate limit connector should be both in the tmp directory\n",
    "kubectl cp -n manager ../../tmp/flink-sql-connector-kafka-ratelimit_2.12-1.14.2.jar $ZEPPELIN_POD:/tmp/flink-sql-connector-kafka-ratelimit_2.12-1.14.2.jar\n",
    "kubectl cp -n manager ../../tmp/nexmark-flink-0.2-SNAPSHOT.jar $ZEPPELIN_POD:/tmp/nexmark-flink-0.2-SNAPSHOT.jar\n",
    "# set Flink interpreter to isolated mode (with default parameters)\n",
    "curl -X PUT  ${MANAGER_NODE}:30088/api/interpreter/setting/flink -H 'Content-Type: application/json' -d @../common/zeppelin-flink-config.json\n",
    "\n",
    "# upload zeppelin notebooks\n",
    "for FILE in ../../experiments/zeppelin/*.zpln; do echo $FILE; \n",
    "    JOB_ID=`curl ${MANAGER_NODE}:30088/api/notebook/import -d @$FILE |jq -r .body`;\n",
    "    curl -X POST ${MANAGER_NODE}:30088/api/notebook/job/${JOB_ID};\n",
    "done\n",
    "# remove from \n",
    "kubectl -n manager exec $ZEPPELIN_POD  -- rm /opt/flink/lib/flink-connector-kafka_2.12-1.14.2.jar\n",
    "kubectl -n manager exec $ZEPPELIN_POD  -- rm /opt/flink/lib/flink-sql-connector-kafka_2.12-1.14.2.jar\n",
    "curl -u \"admin:prom-operator\" -X POST ${MANAGER_NODE}:30300/api/dashboards/import -H 'Content-Type: application/json' -d \"{\\\"Dashboard\\\":$(cat ../common/grafana.json)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
